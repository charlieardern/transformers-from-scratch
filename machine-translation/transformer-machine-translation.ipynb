{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# English to French Translation with a Transformer from Scratch","metadata":{}},{"cell_type":"markdown","source":"## Building Some Modules","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:02.775981Z","iopub.execute_input":"2024-09-18T08:35:02.776459Z","iopub.status.idle":"2024-09-18T08:35:04.502938Z","shell.execute_reply.started":"2024-09-18T08:35:02.776409Z","shell.execute_reply":"2024-09-18T08:35:04.501859Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoder(nn.Module):\n    \n    def __init__(self, n_max, d):\n        super().__init__()\n        self.n_max = n_max\n        self.d = d\n        P = torch.zeros(n_max, d)\n        i = torch.arange(0,n_max, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d, 2).float() * (-math.log(10000.0) / d))\n        P[:, 0::2] = torch.sin(pos * div)\n        P[:, 1::2] = torch.cos(pos * div)\n        P = P.unsqueeze(0)\n        \n        self.register_buffer('P', P, persistent=False) #Make it part of module's state\n        \n        def forward(self, X):\n            X = X + self.P[:, X.size(1)]\n            return X","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:04.504885Z","iopub.execute_input":"2024-09-18T08:35:04.505381Z","iopub.status.idle":"2024-09-18T08:35:04.514129Z","shell.execute_reply.started":"2024-09-18T08:35:04.505323Z","shell.execute_reply":"2024-09-18T08:35:04.512826Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, norm_shape, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(norm_shape)\n        \n    def forward(self, X, Y):\n        return self.layer_norm(self.dropout(Y) + X)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:04.538223Z","iopub.status.idle":"2024-09-18T08:35:04.538956Z","shell.execute_reply.started":"2024-09-18T08:35:04.538743Z","shell.execute_reply":"2024-09-18T08:35:04.538767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionWiseFFN(nn.Module):\n    def __init__(self, hidden_units, output_shape):\n        super().__init__()\n        self.dense1 = nn.LazyLinear(hidden_units) #LazyLinear means you only specify output size\n        self.relu = nn.ReLU()\n        self.dense2 = nn.LazyLinaer(output_shape)\n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:04.534594Z","iopub.status.idle":"2024-09-18T08:35:04.535210Z","shell.execute_reply.started":"2024-09-18T08:35:04.534889Z","shell.execute_reply":"2024-09-18T08:35:04.534918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full Multi-Head Attention Module\nclass MultiheadAttention(nn.Module):\n    \n    def __init__(self, hidden_dim, num_heads, dropout, bias=False):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.W_q = nn.LazyLinear(hidden_dim, bias=bias)\n        self.W_k = nn.LazyLinear(hidden_dim, bias=bias)\n        self.W_v = nn.LazyLinear(hidden_dim, bias=bias)\n        self.W_o = nn.LazyLinear(hidden_dim, bias=bias)\n        \n    def transpose_qkv(self, X):\n        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n        X = X.permute(0, 2, 1, 3)\n        return X.reshape(-1, X.shape[2], X.shape[3])\n\n    def transpose_output(self, X):\n        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n        X = X.permute(0, 2, 1, 3)\n        return X.reshape(X.shape[0], X.shape[1], -1)\n    \n    def forward(self, Q_in, K_in, V_in, valid_lens):\n        Q = self.transpose_qkv(self.W_q(Q_in))\n        K = self.transpose_qkv(self.W_k(K_in))\n        V = self.transpose_qkv(self.W_v(V_in))\n        \n        if valid_lens is not None:\n            valid_lens = torch.repeat_interleave(valid_lens, repeat=self.num_heads, dim=0)\n                 \n        # Sclaled dot-product:\n        d = Q.shape[-1]\n        attn_logits = torch.bmm(Q, K.transpose(1,2)) / math.sqrt(d)\n        \n        # Softmax without mask:\n        if valid_lens is None:\n            self.attention_weights = nn.functional.softmax(attn_logits, dim=-1)\n        # Softmax with mask:\n        else:\n            shape = attn_logits.shape\n            if valid_lens.dim() == 1:\n                valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n            else:\n                valid_lens = valid_lens.reshape(-1)\n            # Creating mask:\n            attn_logits = attn_logits.reshape(-1, shape[-1])    \n            maxlen = attn_logits.size(1)\n            mask = torch.arange((maxlen), dtype=torch.float32, device=attn_logits.device)[None,:] < valid_lens[:,None]\n            # Applying mask:\n            attn_logits[~mask] = 1e-6\n            self.attention_weights = nn.functional.softmax(attn_logits.reshape(shape), dim=-1)\n        \n        # Multiply softmax output with value matrix:\n        output = torch.bmm(self.dropout(self.attention_weights), V)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:18.622534Z","iopub.execute_input":"2024-09-18T08:35:18.622982Z","iopub.status.idle":"2024-09-18T08:35:18.638137Z","shell.execute_reply.started":"2024-09-18T08:35:18.622943Z","shell.execute_reply":"2024-09-18T08:35:18.636910Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Building the Encoder","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, hidden_dim, FFN_hidden_units, num_heads, dropout, use_bias=False):\n        super().__init__()\n        self.MHA = MultiHeadAttention(hidden_dim, num_heads, dropout, use_bias)\n        self.addnorm1 = AddNorm(hidden_dim, dropout)\n        self.FFN = PositionWiseFFN(FFN_hidden_units, hidden_dim)\n        self.addnorm2 = AddNorm(hidden_dim, dropout)\n    def forward(self, X, valid_lens):\n        X = self.addnorm1(X, self.MHA(X,X,X,valid_lens))\n        return self.addnorm2(X, self.FFN(X))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:04.540157Z","iopub.status.idle":"2024-09-18T08:35:04.540601Z","shell.execute_reply.started":"2024-09-18T08:35:04.540399Z","shell.execute_reply":"2024-09-18T08:35:04.540422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, hidden_dim, FFN_hidden_units, num_heads, num_blocks, dropout, use_bias=False):\n        super().__init()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoder = PositionalEncoder(1000, hidden_dim) #max 1000 tokens\n        self.blocks = nn.Sequential()\n        for i in range(num_blocks):\n            self.blocks.add_module(\"block\"+str(i), EncoderBlock(hidden_dim, FFN_hidden_units, num_heads, dropout, use_bias))\n            \n    def forward(self, X, valid_lens):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.hidden_dim))\n        return self.blocks(X)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T08:35:04.543533Z","iopub.status.idle":"2024-09-18T08:35:04.543959Z","shell.execute_reply.started":"2024-09-18T08:35:04.543763Z","shell.execute_reply":"2024-09-18T08:35:04.543784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Decoder","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}