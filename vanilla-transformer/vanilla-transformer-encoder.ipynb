{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building a Transformer Encoder from Scratch\nCharlie Ardern <br>\n<hr>\n\nBelow I've implemented the encoder block from the paper [Attention is all you need, Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). The code was inspired and guided by [this book](https://www.bishopbook.com/) and the free online materials from the deep learning course by the University of Amsterdam. Clearly I'm lacking some annotations here but this is more about just implementing the code. Readers are assumed to already understand how transformers work.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:02:35.312013Z","iopub.execute_input":"2024-09-05T15:02:35.312385Z","iopub.status.idle":"2024-09-05T15:02:35.322645Z","shell.execute_reply.started":"2024-09-05T15:02:35.312345Z","shell.execute_reply":"2024-09-05T15:02:35.321193Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n## Positional Encoding\nBefore feeding data to the encoder, it is often beneficial to encode the positions. This is because the encoder is permutation equivariant so it won't be able to learn from the token positions. The [original paper](https://arxiv.org/abs/1706.03762) makes use of these:\n$$\\mathbf{X} := \\mathbf{X} + \\mathbf{P},$$\nwhere:\n$$ P_{ij} = \\begin{cases} \\sin \\left[\\frac{i}{10000^{j/d}}\\right] \\text{ for even } j\\\\ \\cos \\left[\\frac{i}{10000^{(j-1)/d}}\\right] \\text{ for odd } j\\end{cases}$$\nNow let's implement this in code:","metadata":{}},{"cell_type":"code","source":"class PositionalEncoder(nn.Module):\n    \n    def __init__(self, n_max, d):\n        super().__init__()\n        self.n_max = n_max\n        self.d = d\n        P = torch.zeros(n_max, d)\n        i = torch.arange(0,n_max, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d, 2).float() * (-math.log(10000.0) / d))\n        P[:, 0::2] = torch.sin(pos * div)\n        P[:, 1::2] = torch.cos(pos * div)\n        P = P.unsqueeze(0)\n        \n        self.register_buffer('P', P, persistent=False) #Make it part of module's state\n        \n        def forward(self, X):\n            X = X + self.P[:, X.size(1)]\n            return X","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:02:35.325052Z","iopub.execute_input":"2024-09-05T15:02:35.325469Z","iopub.status.idle":"2024-09-05T15:02:35.336510Z","shell.execute_reply.started":"2024-09-05T15:02:35.325426Z","shell.execute_reply":"2024-09-05T15:02:35.335150Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n# Architecture of the Encoder\nThe structure of the encoder block can be seen in the figure below. (credit [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))\n<br>\n<img src=\"transformer_architecture.svg\">\n\n**The transformer encoder will consist of these key stages:**\n1. Multi-head attention\n2. Add + layer norm\n3. MLP\n4. Add + layer norm\n<br>\n\n**Notation:**\n- $b$ is the batch size\n- $n$ is the sequence length\n- $d$ is the dimension of the embeddings\n<br>\n\nNote that each stage maps $\\mathbb{R}^{b\\times n\\times d} \\to \\mathbb{R}^{b\\times n\\times d}$ so the input and output dimensions are the same.","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n## Multi-Head Attention Class\nThe computation steps and dimensions are shown below:<br>\n<img src=\"MHA_steps.png\" width=\"300px\">\n<br>\nThe MHA block is evidently rather involved so we'll build a class for it:","metadata":{}},{"cell_type":"code","source":"class MHA(nn.Module):\n    \n    def __init__(self, d, h):\n        super().__init__()\n        assert d % h == 0, \"Dimension of embeddings must be divisible by number of heads\"\n        self.d = d\n        self.h = h    \n        self.X_to_QKV = nn.Linear(d, 3*d)\n        self.stacked_heads_to_X = nn.Linear(d, d)\n        self._reset_parameters()\n        \n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.X_to_QKV.weight)\n        nn.init.xavier_uniform_(self.stacked_heads_to_X.weight)\n        self.X_to_QKV.bias.data.fill_(0)\n        self.stacked_heads_to_X.bias.data.fill_(0)\n        \n    def forward(self, X, mask=None):\n        b, n, d = X.size()\n        QKV = self.X_to_QKV(X)\n        QKV = QKV.reshape(b, n, h, 3*d/h)\n        QKV = QKV.permute(0, 2, 1, 3)\n        Q, K, V = QKV.chunk(3, dim=-1)\n        \n        # Scaled dot product attention:\n        logits = torch.matmul(Q,K.transpose(-2,-1))/torch.sqrt(d/h)\n        \n        #Swaps zero values for \n        if mask is not None:\n            logits = attn_logits.masked_fill(mask == 0, -9e15)\n        \n        attention = F.softmax(logits, dim=-1)\n        head_vals = torch.matmul(attention, V)\n        \n        head_vals = head_vals.permute(0, 2, 1, 3)\n        stacked_heads = heav_vals.reshape(b, n, d)\n        X = self.stacked_heads_to_X(stacked_heads)\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:02:35.337934Z","iopub.execute_input":"2024-09-05T15:02:35.338338Z","iopub.status.idle":"2024-09-05T15:02:35.353047Z","shell.execute_reply.started":"2024-09-05T15:02:35.338266Z","shell.execute_reply":"2024-09-05T15:02:35.351829Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n# Constructing the Full Encoder","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    \n    def __init__(self, d, h, mlp_dim, dropout=0.0):\n        super().__init__()\n        \n        # Multi-Head Attention Block\n        self.mha = MHA(d, h)\n        \n        # Multi-Layer Perceptron Block\n        self.mlp = nn.Sequential(\n            nn.Linear(d, mlp_dim),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(mlp_dim, d)\n        )\n        \n        # Layer norms\n        self.Lnorm1 = nn.LayerNorm(d)\n        self.Lnorm2 = nn.LayerNorm(d)\n        \n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X, mask=None):\n        # 1. Attention step\n        X = self.mha(X, mask=mask)\n        \n        # 2. Add + Layer Norm\n        X = X + self.dropout(X)\n        X = self.Lnorm1(X)\n        \n        # 3. MLP\n        X = self.mlp(X)\n        \n        # 4. Add + Layer Norm\n        X = X + self.dropout(X)\n        X = self.Lnorm2(X)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2024-09-05T15:02:35.354826Z","iopub.execute_input":"2024-09-05T15:02:35.355263Z","iopub.status.idle":"2024-09-05T15:02:35.371287Z","shell.execute_reply.started":"2024-09-05T15:02:35.355220Z","shell.execute_reply":"2024-09-05T15:02:35.370075Z"},"trusted":true},"execution_count":16,"outputs":[]}]}
