{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/charlieardern/transformers-from-scratch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T19:15:07.075330Z","iopub.execute_input":"2024-09-04T19:15:07.076923Z","iopub.status.idle":"2024-09-04T19:15:08.253781Z","shell.execute_reply.started":"2024-09-04T19:15:07.076867Z","shell.execute_reply":"2024-09-04T19:15:08.251850Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fatal: destination path 'transformers-from-scratch' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd transformers-from-scratch/vanilla-transformer","metadata":{"execution":{"iopub.status.busy":"2024-09-04T19:16:45.763272Z","iopub.execute_input":"2024-09-04T19:16:45.763764Z","iopub.status.idle":"2024-09-04T19:16:45.773324Z","shell.execute_reply.started":"2024-09-04T19:16:45.763720Z","shell.execute_reply":"2024-09-04T19:16:45.771963Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working/transformers-from-scratch/vanilla-transformer\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building a Transformer from Scratch\nCharlie Ardern <br>\n<hr>\n\n*This is not yet functional. Updates to come soon.*","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first objective is to turn the $X$ matrix into a set of $Q,K,V$ matrices for each head. This can be implemented efficiently with one matrix operation by using `nn.Linear`. We construct a large matrix, of dimension `(batch_size, sequence_length, 3*d*h)`. This will have all required matrices concatenated along `dim=2`.\n- Note that $d$ is the dimension of each token, $h$ is the number of heads and the 3 comes about as we are consturcting 3 matrices (Q,K,V) for each head. (See the image below)\n- Note that `nn.Linear` is designed to compute operations on a vector so it will perform the same operation on each token of each batch.<br>\n\nHere we see the operation carried out by the linear layer:\n<br>\n\n<br>\nAnd looking more closely at the output:\n<br>\n\n","metadata":{}},{"cell_type":"markdown","source":"**The transformer encoder will consist of these key stages:**\n1. Positional encoding\n2. Multi-head attention\n3. Add + layer norm\n4. MLP\n5. Add + layer norm","metadata":{}},{"cell_type":"code","source":"class MHA(nn.Module):\n    \n    def __init__(self, d, h):\n        super().__init__()\n        assert d % h == 0, \"Dimension of embeddings must be divisible by number of heads\"\n        self.d = d\n        self.h = h    \n        self.X_to_QKV = nn.Linear(d, 3*d)\n        self.stacked_heads_to_X = nn.Linear(d, d)\n        self._reset_parameters()\n        \n    def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.X_to_QKV.weight)\n        nn.init.xavier_uniform_(self.stacked_heads_to_X.weight)\n        self.X_to_QKV.bias.data.fill_(0)\n        self.stacked_heads_to_X.bias.data.fill_(0)\n        \n    def forward(self, x):\n        b, n, d = x.size()\n        QKV = self.X_to_QKV(X)\n        QKV = QKV.reshape(b, n, h, 3*d/h)\n        QKV = QKV.permute(0, 2, 1, 3)\n        Q, K, V = QKV.chunk(3, dim=-1)\n        \n        # Scaled dot product attention:\n        logits = torch.matmul(Q,K.transpose(-2,-1))/torch.sqrt(d/h)\n        #can implement masking here later\n        attention = F.softmax(logits, dim=-1)\n        head_vals = torch.matmul(attention, V)\n        \n        head_vals = head_vals.permute(0, 2, 1, 3)\n        stacked_heads = heav_vals.reshape(b, n, d)\n        X = self.stacked_heads_to_X(stacked_heads)\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
